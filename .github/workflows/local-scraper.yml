name: Local Scraper (GitHub-hosted)

on:
  schedule:
    - cron: '*/5 * * * *'  # Every 5 minutes (UTC)
  workflow_dispatch:  # Manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:  
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Run local scraper (Puppeteer on GitHub)
        run: node local-scraper.js
        
      - name: Upload to Azure Blob Storage & Daily Logging
        if: success()
        run: |
          npm install @azure/storage-blob --save
          node << 'EOF'
          const fs = require('fs');
          const { BlobServiceClient } = require('@azure/storage-blob');
          
          const connStr = process.env.KOGB_STORAGE_CONNECTION_STRING;
          if (!connStr) {
            console.error('ÔØî Error: KOGB_STORAGE_CONNECTION_STRING secret not set!');
            process.exit(1);
          }
          
          async function uploadBlob() {
            try {
              const client = BlobServiceClient.fromConnectionString(connStr);
              const cacheContainer = client.getContainerClient('cache');
              const dataContainer = client.getContainerClient('data');
              
              // 1. Upload latest_with_trend.json
              const latestData = fs.readFileSync('local-scrape.json', 'utf-8');
              const latestBlobClient = cacheContainer.getBlockBlobClient('latest_with_trend.json');
              await latestBlobClient.upload(latestData, Buffer.byteLength(latestData));
              
              const parsed = JSON.parse(latestData);
              console.log('Ô£ô Uploaded cache/latest_with_trend.json');
              console.log('  Vendors: ' + parsed.vendors.length);
              console.log('  Timestamp: ' + parsed.scrapedAt);
              
              // 2. Daily logging (every 30 minutes)
              const LOG_INTERVAL_SECONDS = 1800; // 30 minutes
              const now = new Date();
              const turkeyTime = new Date(now.toLocaleString('en-US', { timeZone: 'Europe/Istanbul' }));
              const year = turkeyTime.getFullYear();
              const month = String(turkeyTime.getMonth() + 1).padStart(2, '0');
              const day = String(turkeyTime.getDate()).padStart(2, '0');
              const dateFileName = year + '-' + month + '-' + day + '.json';
              
              let dailyData = [];
              try {
                const downloadResp = await dataContainer.getBlobClient(dateFileName).download();
                const chunks = [];
                for await (const chunk of downloadResp.readableStreamBody) {
                  chunks.push(chunk);
                }
                const content = Buffer.concat(chunks).toString('utf-8');
                dailyData = JSON.parse(content);
              } catch (err) {
                // File doesn't exist yet, start fresh
                console.log('  Ô£å Creating new daily file: ' + dateFileName);
              }
              
              // Check if we should log (based on interval)
              let shouldLog = true;
              if (dailyData.length > 0) {
                const lastLogTime = new Date(dailyData[dailyData.length - 1].scrapedAt);
                const diffSeconds = (now.getTime() - lastLogTime.getTime()) / 1000;
                if (diffSeconds < LOG_INTERVAL_SECONDS) {
                  shouldLog = false;
                  console.log('  Ô£á Skipping daily log (last log: ' + Math.round(diffSeconds) + 's ago, interval: ' + LOG_INTERVAL_SECONDS + 's)');
                }
              }
              
              // Append to daily file if interval passed
              if (shouldLog) {
                dailyData.push(parsed);
                const dailyBlobClient = dataContainer.getBlockBlobClient(dateFileName);
                const dailyContent = JSON.stringify(dailyData, null, 2);
                await dailyBlobClient.upload(dailyContent, Buffer.byteLength(dailyContent));
                console.log('Ô£ô Appended to data/' + dateFileName);
                console.log('  Ôªæ Total entries today: ' + dailyData.length);
              }
              
            } catch (error) {
              console.error('Ô£ù Upload failed: ' + error.message);
              console.error(error);
              process.exit(1);
            }
          }
          uploadBlob();
          EOF
        env:
          KOGB_STORAGE_CONNECTION_STRING: ${{ secrets.KOGB_STORAGE_CONNECTION_STRING }}
        
      - name: Workflow Status
        if: success()
        run: echo "✓ Scraper ran successfully and updated Azure Blob Storage"
